import os
import shutil
import time

from app_modules.overwrites import postprocess
from app_modules.presets import *
from clc.config import LangChainCFG
from clc.langchain_application import LangChainApplication

config = LangChainCFG()
application = LangChainApplication(config)

application.source_service.init_source_vector()

def get_file_list():
    if not os.path.exists("docs"):
        return []
    return [f for f in os.listdir("docs")]


file_list = get_file_list()

def load_new_file(file, history):
    if file is None:
        msg_status = "æ²¡æœ‰æ”¶åˆ°ä¸Šä¼ æ–‡ä»¶"
    else:
        filename = os.path.basename(file.name)
        if os.path.exists("docs/" + filename):
            # file_listé¦–ä½æ’å…¥æ–°ä¸Šä¼ çš„æ–‡ä»¶
            file_list.insert(0, filename)
            application.source_service.add_document("docs/" + filename)
            msg_status = f'{filename} æ–‡ä»¶å·²æˆåŠŸåŠ è½½'
        else:
            msg_status = f'{filename} æ–‡ä»¶ä¸å­˜åœ¨'
    return '', history + [[None, msg_status]]

def upload_file(file):
    if not os.path.exists("docs"):
        os.mkdir("docs")
    filename = os.path.basename(file.name)
    shutil.move(file.name, "docs/" + filename)
    # file_listé¦–ä½æ’å…¥æ–°ä¸Šä¼ çš„æ–‡ä»¶
    #file_list.insert(0, filename)
    #application.source_service.add_document("docs/" + filename)
    return gr.Dropdown.update(choices=file_list, value=filename)


def set_knowledge(kg_name, history):
    try:
        application.source_service.load_vector_store(config.kg_vector_stores[kg_name])
        msg_status = f'{kg_name}çŸ¥è¯†åº“å·²æˆåŠŸåŠ è½½'
    except Exception as e:
        print(e)
        msg_status = f'{kg_name}çŸ¥è¯†åº“æœªæˆåŠŸåŠ è½½'
    return history + [[None, msg_status]]


def clear_session():
    return '', None

def show_user_msg(user_message, history):
    cur_history = history
    if history is None:
        cur_history = []
    cur_history = cur_history + [[user_message, None]]
    return user_message, cur_history

def predict(input,
            large_language_model,
            embedding_model,
            top_k,
            use_web,
            use_pattern,
            history=None):
    # print(large_language_model, embedding_model)
    print(input)
    if history == None:
        history = []

    if use_web == 'ä½¿ç”¨':
        web_content = application.source_service.search_web(query=input)
    else:
        web_content = ''
    search_text = ''
    if use_pattern == 'æ¨¡å‹é—®ç­”':
        result = application.get_llm_answer(query=input, web_content=web_content)
        history.append((input, result))
        if use_web == 'ä½¿ç”¨':
            search_text += "----------ã€ç½‘ç»œæ£€ç´¢å†…å®¹ã€‘-----------\n"
        search_text += web_content
        return '', history, search_text

    else:
        resp = application.get_knowledge_based_answer(
            query=input,
            history_len=1,
            temperature=0.1,
            top_p=0.9,
            top_k=top_k,
            web_content=web_content,
            chat_history=history
        )
        history.append((input, resp['result']))
        for idx, source in enumerate(resp['source_documents'][:4]):
            sep = f'----------ã€æœç´¢ç»“æœ{idx + 1}ï¼šã€‘---------------\n'
            search_text += f'{sep}\n{source.page_content}\n\n'
        print(search_text)
        search_text += "----------ã€ç½‘ç»œæ£€ç´¢å†…å®¹ã€‘-----------\n"
        search_text += web_content
        return '', history, search_text

# show stream response
def show_stream_response(history):
    if history is None:
        print("Exception history is None")
        return
    result = history[-1][1]
    message = history[-1][0]
    history.pop()
    history = history + [[message, '']]
    for char in result:
        history[-1][1] += char
        yield (history, history)
        time.sleep(0.03)

def change_model_type(large_language_model):
    application.change_model(large_language_model)
    return large_language_model

with open("assets/custom.css", "r", encoding="utf-8") as f:
    customCSS = f.read()
with gr.Blocks(css=customCSS, theme=small_and_beautiful_theme) as demo:
    gr.Markdown("""<h1><center>Chinese-LangChain-ChatGLM</center></h1>
        <center><font size=3>
        </center></font>
        """)
    state = gr.State()

    with gr.Row():
        with gr.Column(scale=1):
            embedding_model = gr.Dropdown([
                "text2vec-base"
            ],
                label="Embedding model",
                value="text2vec-base")

            large_language_model = gr.Dropdown(
                [
                    config.chatglm_model_name,
                    config.qwen_model_name
                ],
                label="large language model",
                value=config.chatglm_model_name)

            top_k = gr.Slider(1,
                              20,
                              value=4,
                              step=1,
                              label="æ£€ç´¢top-kæ–‡æ¡£",
                              interactive=True)

            use_web = gr.Radio(["ä½¿ç”¨", "ä¸ä½¿ç”¨"], label="web search",
                               info="æ˜¯å¦ä½¿ç”¨ç½‘ç»œæœç´¢ï¼Œä½¿ç”¨æ—¶ç¡®ä¿ç½‘ç»œé€šå¸¸",
                               value="ä¸ä½¿ç”¨"
                               )
            use_pattern = gr.Radio(
                [
                    'æ¨¡å‹é—®ç­”',
                    'çŸ¥è¯†åº“é—®ç­”',
                ],
                label="æ¨¡å¼",
                value='æ¨¡å‹é—®ç­”',
                interactive=True)

            kg_name = gr.Radio(list(config.kg_vector_stores.keys()),
                               label="çŸ¥è¯†åº“",
                               value=None,
                               info="ä½¿ç”¨çŸ¥è¯†åº“é—®ç­”ï¼Œè¯·åŠ è½½çŸ¥è¯†åº“",
                               interactive=True)
            set_kg_btn = gr.Button("åŠ è½½çŸ¥è¯†åº“")

            file = gr.File(label="å°†æ–‡ä»¶ä¸Šä¼ åˆ°çŸ¥è¯†åº“åº“ï¼Œå†…å®¹è¦å°½é‡åŒ¹é…",
                           visible=True,
                           file_types=['.txt', '.md', '.docx', '.pdf', 'doc']
                           )

            set_file_btn = gr.Button("åŠ è½½æ–°æ–‡ä»¶")

        with gr.Column(scale=4):
            with gr.Row():
                chatbot = gr.Chatbot(label='Chinese-LangChain-ChatGLM').style(height=600)
            with gr.Row():
                message = gr.Textbox(label='è¯·è¾“å…¥é—®é¢˜')
            with gr.Row():
                clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                send = gr.Button("ğŸš€ å‘é€")
            with gr.Row():
                gr.Markdown("""æé†’ï¼š<br>
                                        [Chinese-LangChain-chatglm_cpp](https://github.com/icemoon/Chinese-LangChain-chatglm_cpp) <br>
                                        æœ‰ä»»ä½•ä½¿ç”¨é—®é¢˜[Github IssueåŒº](https://github.com/icemoon/Chinese-LangChain-chatglm_cpp)è¿›è¡Œåé¦ˆ. <br>
                                        """)
        with gr.Column(scale=2):
            search = gr.Textbox(label='æœç´¢ç»“æœ')

        # ============= è§¦å‘åŠ¨ä½œ=============
        # ä¸‹æ‹‰æ¡†ä¿®æ”¹æ¨¡å‹
        large_language_model.change(change_model_type,
                                    inputs = [large_language_model],
                                    outputs= [large_language_model])

        file.upload(upload_file,
                    show_progress=True,
                    inputs=file,
                    outputs=None)

        set_file_btn.click(
            load_new_file,
            show_progress=True,
            inputs=[file, chatbot],
            outputs=[message, chatbot]
        )

        set_kg_btn.click(
            set_knowledge,
            show_progress=True,
            inputs=[kg_name, chatbot],
            outputs=chatbot
        )

        # æ¸…ç©ºå†å²å¯¹è¯æŒ‰é’® æäº¤
        clear_history.click(fn=clear_session,
                            inputs=[],
                            outputs=[chatbot, state],
                            queue=False)

        # å‘é€æŒ‰é’® æäº¤
        send.click(show_user_msg,
                    [message, state],
                    [message, chatbot],
                    queue=False).then(
        predict,
        [message, large_language_model, embedding_model,
        top_k, use_web, use_pattern, state],
        [message, state, search],
        queue=True).then(
            show_stream_response,
            state,
            [chatbot, state],
            queue=True
        )

        # è¾“å…¥æ¡† å›è½¦
        message.submit(show_user_msg,
                    [message, state],
                    [message, chatbot],
                    queue=False).then(
        predict,
        [message, large_language_model, embedding_model,
        top_k, use_web, use_pattern, state],
        [message, state, search],
        queue=True).then(
            show_stream_response,
            state,
            [chatbot, state],
            queue=True
        )

demo.queue(concurrency_count=2).launch(
    server_name='0.0.0.0',
    server_port=8888,
    share=False,
    show_error=True,
    debug=True,
    enable_queue=True,
    inbrowser=True,
)
